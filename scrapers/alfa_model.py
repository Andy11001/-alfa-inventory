import requests
from bs4 import BeautifulSoup
import re
import csv
import os

# Konfiguracja
BASE_URL = "https://www.alfaromeo.pl"
OUTPUT_FILE = os.path.join(os.path.dirname(os.path.dirname(__file__)), "data", "alfa_model_feed.csv")

def get_dynamic_model_urls(session):
    """
    Automatycznie wykrywa linki do modeli ze strony zbiorczej.
    """
    hub_url = f"{BASE_URL}/modele"
    print(f"üïµÔ∏è  Dynamiczne wykrywanie modeli z: {hub_url}")
    found_urls = set()
    
    try:
        r = session.get(hub_url, timeout=15)
        soup = BeautifulSoup(r.text, 'html.parser')
        
        # Heurystyka: Szukamy link√≥w zawierajƒÖcych '/modele/' ale nie bƒôdƒÖcych samym hubem
        for a in soup.find_all('a', href=True):
            href = a['href']
            if "/modele/" in href and href != "/modele" and "dostepne" not in href:
                # Wykluczenia "≈õmieciowych" link√≥w
                if any(x in href for x in ["wersje-limitowane", "quadrifoglio-100", "competizione", "luna-rossa"]):
                    continue
                    
                full_url = href if href.startswith("http") else f"{BASE_URL}{href}"
                found_urls.add(full_url)
                
    except Exception as e:
        print(f"‚ùå B≈ÇƒÖd podczas wykrywania modeli: {e}")
        # Fallback do kluczowych modeli w razie awarii discovery
        return [
            f"{BASE_URL}/modele/junior-ibrida",
            f"{BASE_URL}/modele/tonale",
            f"{BASE_URL}/modele/stelvio",
            f"{BASE_URL}/modele/giulia"
        ]
        
    print(f"‚úÖ Znaleziono {len(found_urls)} stron modeli.")
    return sorted(list(found_urls))

def extract_model_code(html_text):
    """
    Pr√≥buje wyciƒÖgnƒÖƒá kod modelu (np. 622) z tre≈õci strony.
    Szuka w JSON-ie vehicleID lub linkach konfiguratora.
    """
    # 1. Metoda najpewniejsza: vehicleID w JSON
    # Pattern: "vehicleID" : "0836223" (gdzie 083 to marka, 622 to model, 3 to seria/wersja)
    # Interesuje nas ≈õrodkowa czƒô≈õƒá '622'
    match_json = re.search(r'"vehicleID"\s*:\s*"083(\d{3})\d"', html_text)
    if match_json:
        return match_json.group(1)

    # 2. Metoda link√≥w konfiguratora (fallback)
    # Pattern: /konfigurator/#/l/pl/pl/622/
    match_link = re.search(r'konfigurator.*?(?:/|%2F)(\d{3})(?:/|%2F)', html_text)
    if match_link:
        return match_link.group(1)
        
    return ""

def clean_title(title):
    """
    Usuwa parametry silnikowe i naprawia formatowanie.
    """
    t = title.replace(" - ", " ")
    
    t = re.sub(r'\b\d\.\d\s+', '', t) 
    t = re.sub(r'GME|JTDM|V6', '', t, flags=re.I)
    t = re.sub(r'\d+\s*KM', '', t, flags=re.I)
    t = re.sub(r'\d+\s*kWh', '', t, flags=re.I)
    t = re.sub(r'Mild-Hybrid', '', t, flags=re.I)
    t = re.sub(r'AWD', '', t, flags=re.I)
    t = t.replace("(bev)", "").replace("(BEV)", "")
    
    if "PLUG-IN" in t.upper():
        t = re.sub(r'PLUG-IN\s*HYBRID', 'PHEV', t, flags=re.I)
        t = t.replace("Plug-in", "PHEV")
    
    words = t.split()
    cleaned_words = []
    for w in words:
        upper_w = w.upper()
        if upper_w in ["Q4", "PHEV", "TI"]:
            cleaned_words.append(upper_w.capitalize() if upper_w == "TI" else upper_w)
        elif upper_w in ["IBRIDA", "ELETTRICA"]:
            cleaned_words.append(w.capitalize())
        else:
            cleaned_words.append(w.capitalize())
            
    t = " ".join(cleaned_words)
    
    if "Alfa Romeo" not in t:
        t = f"Alfa Romeo {t}"

    t = re.sub(r'\b(\w+)\s+\1\b', r'\1', t, flags=re.I) 
    return t.strip()

def clean_price(price_str):
    if not price_str: return 0
    clean = re.sub(r'[^\d]', '', price_str)
    return int(clean) if clean else 0

def extract_offers_from_text(text, model_code=""):
    offers = []
    pattern = r"modelu\s+(?P<model_full>[^:]+?):\s*cena.+?brutto\s*(?P<price>[\d\s\xa0]+)\s*z≈Ç.+?okres.+?(?P<months>\d+)\s*mies.+?wp≈Çata.+?(?P<down_payment>\d+)\s*%.+?netto:?\s*(?P<installment>[\d\s\xa0]+)\s*z≈Ç(?P<disclaimer>(?:(?!modelu).){0,400})"
    
    normalized_text = text.replace('\xa0', ' ')
    matches = re.finditer(pattern, normalized_text, re.IGNORECASE | re.DOTALL)
    
    for match in matches:
        raw_title = match.group("model_full").strip().replace("\n", " ")
        full_title = clean_title(raw_title)
        
        raw_disclaimer = match.group("disclaimer").strip()
        if "." in raw_disclaimer:
            raw_disclaimer = raw_disclaimer.rsplit(".", 1)[0] + "."
        
        offer = {
            "vehicle_id": f"MODEL-{abs(hash(full_title))}",
            "title": full_title,
            "price_brutto": clean_price(match.group("price")),
            "installment_netto": clean_price(match.group("installment")),
            "months": match.group("months"),
            "down_payment_pct": match.group("down_payment"),
            "disclaimer": raw_disclaimer,
            "model_code": model_code # Nowe pole!
        }
        offers.append(offer)
    return offers

def main():
    print(f"üöÄ Rozpoczynam pobieranie ofert modelowych (Auto-Discovery)...")
    session = requests.Session()
    session.headers.update({"User-Agent": "Mozilla/5.0"})
    
    # Krok 1: Wykrycie URLi
    target_urls = get_dynamic_model_urls(session)
    
    all_found_offers = []
    processed_titles = set()

    for url in target_urls:
        print(f"‚û°Ô∏è Analiza: {url}")
        try:
            r = session.get(url, timeout=15)
            if r.status_code == 200:
                html_text = r.text
                
                # Krok 2: Ekstrakcja kodu modelu (MVSS base)
                model_code = extract_model_code(html_text)
                if model_code:
                    print(f"   ‚ÑπÔ∏è Wykryto kod modelu: {model_code}")
                
                # Ekstrakcja ofert
                clean_text = BeautifulSoup(html_text, 'html.parser').get_text(" ", strip=True)
                offers = extract_offers_from_text(clean_text, model_code)
                
                for off in offers:
                    # Unikanie duplikat√≥w (czasem ta sama oferta jest na kilku podstronach)
                    if off['title'] not in processed_titles:
                        all_found_offers.append(off)
                        processed_titles.add(off['title'])
                        print(f"   ‚úÖ Oferta: {off['title']} ({off['installment_netto']} z≈Ç/mc)")
        except Exception as e: 
            print(f"   ‚ö†Ô∏è B≈ÇƒÖd: {e}")
            continue

    if all_found_offers:
        # Dodajemy 'model_code' do nag≈Ç√≥wk√≥w
        fieldnames = ["vehicle_id", "title", "price_brutto", "installment_netto", "months", "down_payment_pct", "disclaimer", "model_code"]
        with open(OUTPUT_FILE, 'w', newline='', encoding='utf-8') as f:
            writer = csv.DictWriter(f, fieldnames=fieldnames)
            writer.writeheader()
            writer.writerows(all_found_offers)
        print(f"\nüíæ Zapisano {len(all_found_offers)} ofert do pliku po≈õredniego.")
        
        # Trigger generation of the final feed
        try:
            import generate_full_model_feed
            print("üöÄ Uruchamianie generatora finalnego feedu...")
            generate_full_model_feed.main()
        except ImportError:
            print("‚ö†Ô∏è Nie mo≈ºna zaimportowaƒá generate_full_model_feed.")
        except Exception as e:
            print(f"‚ùå B≈ÇƒÖd podczas generowania finalnego feedu: {e}")
    else:
        print("‚ùå Nie znaleziono ≈ºadnych ofert.")

if __name__ == "__main__":
    main()