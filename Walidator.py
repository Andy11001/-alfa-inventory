# -*- coding: utf-8 -*-
import csv
import sys
from urllib.parse import urlparse

# Konfiguracja pliku do sprawdzenia
FILENAME = "alfa_romeo_feed.csv"

# Pola wymagane (krytyczne dla feedu)
REQUIRED_HEADERS = [
    "vehicle_id", 
    "title", 
    "make", 
    "model", 
    "year", 
    "link", 
    "image_link", 
    "offer_type",
    "amount_price" # WaÅ¼ne dla ofert leasingowych
]

def is_valid_url(url):
    try:
        result = urlparse(url)
        return all([result.scheme, result.netloc]) and result.scheme in ['http', 'https']
    except:
        return False

def check_csv(filename):
    print(f"--- ROZPOCZYNAM WALIDACJÄ˜ PLIKU: {filename} ---\n")
    
    try:
        with open(filename, mode='r', encoding='utf-8') as f:
            reader = csv.DictReader(f)
            headers = reader.fieldnames
            rows = list(reader)
            
            # 1. Sprawdzenie nagÅ‚Ã³wkÃ³w
            print("[1/5] Sprawdzanie nagÅ‚Ã³wkÃ³w...")
            missing_headers = [h for h in REQUIRED_HEADERS if h not in headers]
            if missing_headers:
                print(f"âŒ BÅÄ„D: Brakuje wymaganych kolumn: {missing_headers}")
                return False
            print("âœ… NagÅ‚Ã³wki OK.")

            # 2. Sprawdzenie czy sÄ… dane
            print(f"\n[2/5] Sprawdzanie zawartoÅ›ci (znaleziono {len(rows)} wierszy)...")
            if len(rows) == 0:
                print("âŒ BÅÄ„D: Plik CSV jest pusty (poza nagÅ‚Ã³wkiem)! Scraper nie znalazÅ‚ Å¼adnych ofert.")
                return False
            print("âœ… Plik zawiera dane.")

            # 3. Sprawdzenie unikalnoÅ›ci ID i pustych pÃ³l
            print("\n[3/5] Analiza wierszy (ID, puste pola, formaty)...")
            seen_ids = set()
            errors = 0
            warnings = 0

            for i, row in enumerate(rows, start=1):
                # Check Vehicle ID
                v_id = row.get("vehicle_id", "").strip()
                if not v_id:
                    print(f"âŒ Wiersz {i}: Puste vehicle_id!")
                    errors += 1
                elif v_id in seen_ids:
                    print(f"âŒ Wiersz {i}: Zduplikowane vehicle_id: '{v_id}'")
                    errors += 1
                else:
                    seen_ids.add(v_id)

                # Check Required Fields
                for field in REQUIRED_HEADERS:
                    if not row.get(field, "").strip():
                        print(f"âŒ Wiersz {i} (ID: {v_id}): Puste pole '{field}'")
                        errors += 1

                # Check URLs
                if not is_valid_url(row.get("link", "")):
                    print(f"âŒ Wiersz {i} (ID: {v_id}): NieprawidÅ‚owy link do oferty")
                    errors += 1
                if not is_valid_url(row.get("image_link", "")):
                    print(f"âš ï¸ Wiersz {i} (ID: {v_id}): NieprawidÅ‚owy link do zdjÄ™cia")
                    warnings += 1

                # Check Price/Rate presence
                price = row.get("amount_price", "")
                if "zÅ‚" not in price and "PLN" not in price:
                     print(f"âš ï¸ Wiersz {i} (ID: {v_id}): Cena '{price}' moÅ¼e nie zawieraÄ‡ waluty.")
                     warnings += 1

            # 4. Podsumowanie
            print("\n--- RAPORT KOÅƒCOWY ---")
            if errors == 0:
                print("ğŸŸ¢ WALIDACJA POZYTYWNA. Plik gotowy do importu.")
                if warnings > 0:
                    print(f"âš ï¸ ZwrÃ³Ä‡ uwagÄ™ na {warnings} ostrzeÅ¼eÅ„ powyÅ¼ej.")
            else:
                print(f"ğŸ”´ WALIDACJA NEGATYWNA. Znaleziono {errors} bÅ‚Ä™dÃ³w krytycznych.")

    except FileNotFoundError:
        print(f"âŒ BÅÄ„D: Nie znaleziono pliku {filename}. Uruchom najpierw scraper.")
    except Exception as e:
        print(f"âŒ BÅÄ„D KRYTYCZNY: {e}")

if __name__ == "__main__":
    check_csv(FILENAME)